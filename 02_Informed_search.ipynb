{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5070606",
   "metadata": {},
   "source": [
    "# Lab02: Informed search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce290cdf",
   "metadata": {},
   "source": [
    "Overview of the lab: \n",
    "1. Recap from lab 01\n",
    "2. Heuristics\n",
    "3. Greedy\n",
    "4. Beam search \n",
    "5. Hill climbing\n",
    "6. A* and optimality of A*\n",
    "7. Choosing a heuristic\n",
    "8. IDA*\n",
    "9. Summary\n",
    "10. Simulated Annealing\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eef0959",
   "metadata": {},
   "source": [
    "## 1. Recap from lab 01: \n",
    "Strategies are evaluated along the following dimensions:\n",
    "  * **Completeness:** does it always find solution if one exists?\n",
    "  * **Optimality**: does it always find a least-cost solution?\n",
    "  * **Time complexity**: bumber of nodes generated/expanded. We are interested in the worst case scenario\n",
    "  * **Space Complexity**: maximum number of nodes in memory\n",
    "\n",
    "Time and space complexity are measured in terms of:\n",
    "* **b** ‚Äì maximum branching factor of the search tree\n",
    "* **d** ‚Äì depth of the least-cost solution\n",
    "* **m** ‚Äì maximum depth of the state space (may be ‚àû)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cb1a59",
   "metadata": {},
   "source": [
    "**Uninformed Search:** Uninformed strategies use only the information available in the problem definition.\n",
    "\n",
    "**Informed Search:** Informed strategies have information on the goal state which helps in more efficient searching. This information is obtained by a function (heuristic) that estimates how close a state is to the goal state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7995de6",
   "metadata": {},
   "source": [
    "Informed Search algorithms: \n",
    "- **Greedy** \n",
    "  - Beam Search \n",
    "  - Hill Climbing\n",
    "- **A*** \n",
    "  - Memory-Bounded A* \n",
    "  - iterative Deepening A* (IDA*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3740e601",
   "metadata": {},
   "source": [
    "## 2. Heuristic\n",
    "Heuristics are the driving force that allow estimation of distance to goal states - they‚Äôre functions that take in a state as input and output a corresponding estimate. The computation performed by such a function is specific to the search problem being solved.\n",
    "\n",
    "With heuristics, it becomes very easy to implement logic in our agent that enables them to ‚Äúprefer‚Äù expanding states that are estimated to be closer to goal states when deciding which action to perform. \n",
    "\n",
    "A function $h(n)$ that estimates the cost (or distance) from a given state n to a goal state. It provides problem-specific guidance to search algorithms to prefer more promising states and reduce the search effort."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bbca7a",
   "metadata": {},
   "source": [
    "## 3. Greedy\n",
    "\n",
    "**Description**: Greedy search is a strategy for exploration that always selects the frontier node with the lowest heuristic value for expansion, which corresponds to the state it believes is nearest to a goal.\n",
    "\n",
    "**Frontier representation**:  Greedy search operates identically to UCS, with a priority queue \n",
    "\n",
    "Greedy search is not guaranteed to find a goal state if one exists, nor is it optimal, particularly in cases where a very bad heuristic function is selected. It generally acts fairly unpredictably from scenario to scenario, and can range from going straight to a goal state to acting like a badly-guided DFS and exploring all the wrong areas.\n",
    "\n",
    "-----\n",
    "- **Complete**: No, but Complete in finite space with repeated-state checking\n",
    "-----\n",
    "- **Optimal** : No \n",
    "-----\n",
    "- **Time complexity** : $O(b^m)$ - a misleading \"bad\" heuristic may lead us to possibly explore all nodes\n",
    "-----\n",
    "- **Space complexity** : $O(b^m)$ - keep all nodes in memeory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55349588",
   "metadata": {},
   "source": [
    "## 4. Beam search (Local Beam Search)\n",
    "= greedy best-first search with queue limit $l$, i.e., keeps $l$ nodes in the queue\n",
    "\n",
    "-----\n",
    "- **Complete**: No, local search\n",
    "-----\n",
    "- **Optimal** : No \n",
    "-----\n",
    "- **Time complexity** : $O(bml)$ \n",
    "\n",
    "**Expansion:** The algorithm only expands the $l$ nodes selected from the previous level.\n",
    "\n",
    "**Generation of Successors:** Each of the $l$ nodes generates $b$ successors (children).The total number of new candidates generated at this level is $l \\times b$.\n",
    "\n",
    "**Heuristic Evaluation/Scoring:** The algorithm calculates the score for all $l \\cdot b$ candidates.This step takes time proportional to $\\mathcal{O}(B \\cdot b)$.\n",
    "\n",
    "**Pruning (Selection):** The algorithm selects the best $B$ candidates out of the $B \\cdot b$ total to carry forward to the next level.Sorting or selection takes time, often $\\mathcal{O}(l \\cdot b \\log(l \\cdot b))$ if sorting is done, or faster if efficient selection algorithms (like Quickselect or a min-heap) are used. In many common search contexts, *this selection step is often simplified or amortized, or assumed to be minor compared to the expansion cost, leading to the linear complexity*.\n",
    "\n",
    "**Overall Time Complexity**: Since the process is repeated for $m$ levels, the total time complexity is: ${O}(m \\cdot l \\cdot b)$\n",
    " \n",
    "----\n",
    "- **Space complexity** : $O(bl)$ - linear space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c37bd04",
   "metadata": {},
   "source": [
    "## 5. Hill Climbing\n",
    "= greedy best-first search with queue limit l = 1, i.e., keeps only the best node\n",
    "\n",
    "-----\n",
    "- **Complete**: No\n",
    "----\n",
    "- **Optimal** : No \n",
    "-----\n",
    "- **Time complexity** : $O(bm)$ \n",
    "----\n",
    "\n",
    "- **Space Complexity** : $O(b)$\n",
    "----\n",
    "\n",
    "* **Random-restart hill climbing overcomes local maxima** ‚Äì trivially complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a931763",
   "metadata": {},
   "source": [
    "### Hill Climbing and Beam Search\n",
    "Key use case: optimization problems\n",
    "* Beam Search and Hill Climbing are better suited for optimization rather than pathfinding.\n",
    "\n",
    "* In optimization, the focus is on iteratively improving asolution, rather than searching for a direct path between A and B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13857e1b",
   "metadata": {},
   "source": [
    "Applications of Beam Search in AI üîé\n",
    "- Natural Language Processing (NLP):\n",
    "  - Machine Translation: Translating a sentence from a source language to a target language by selecting the sequence of words with the highest probability.\n",
    "  - Text Summarization: Generating a coherent and concise summary by selecting the most probable sequence of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4ea539",
   "metadata": {},
   "source": [
    "## 6. A* and optimality of A*\n",
    "\n",
    "**Idea:** avoid expanding paths that are already expensive \n",
    "* Evaluation function $f(n) = g(n) + h(n)$\n",
    "   * $g(n)$ = cost so far to reach n\n",
    "   * $h(n)$ = estimated cost from n to goal \n",
    "   * $f(n)$ = estimated total cost of path trough **n** to goal\n",
    "* A* search uses an ***admissible heuristic***  i.e., $h(n) ‚â§ h^*(n)$ where $h^*(n)$ is the true cost from n. (Also require $h(n) ‚â• 0$ , so $h(G) = 0$ for any goal $G$.). A heuristic $h(n)$ is admissible if it never overestimates the true cost of reaching the goal state $G$ from any node $n$.\n",
    "   * E.g., $h_{SLD}$  never overestimates the actual road distance\n",
    "\n",
    "\n",
    "![a_star_graph](images/a-star-example-graph_corrected%20(1).png)\n",
    "\n",
    "-----\n",
    "- **Complete**: Yes\n",
    "----\n",
    "- **Optimal** : Yes, cannot expand $f_{i+1}$ unil $f_i$ is finished \n",
    "* A* expands all nodes with f(n) < C*\n",
    "* A* expands some nodes with f(n) = C*\n",
    "* A* expands no nodes with f(n) > C*\n",
    "-----\n",
    "- **Time complexity** : $O(b^d)$ \n",
    "----\n",
    "\n",
    "- **Space Complexity** : $O(b^d)$\n",
    "\n",
    "-----\n",
    "\n",
    "***Theorem*** A* Search is Optimal\n",
    "* If $h(n)$ is admissible, A* using TREE-SEARCH is optimal\n",
    "* If $h(n)$ is consistent, A* using GRAPH-SEARCH is optimal\n",
    "  * **Optimality:** A consistent heuristic guarantees that once a node is expanded, its cost will not be improved upon. Thus, the path found by A* will be the shortest possible.\n",
    "  * **Efficiency:** Because consistent heuristics prevent the re-expansion of nodes, they help reduce unnecessary evaluations, leading to a more efficient search process.\n",
    "\n",
    "*If h(n) is consistent, then it is also admissible; however, the converse is not necessarily true.*\n",
    "\n",
    "Suppose some suboptimal goal $G_2$ has been generated and is in the queue. Let n be an unexpanded node on a shortest path to an optimal goal $G_1$.\n",
    "\n",
    "![optimality](images/optimality.jpg)\n",
    "\n",
    "$f(G_2) = g(G_2)$, the actual path to $G_2$, since $h(G_2) = 0$\n",
    "\n",
    "$f(G_2) > g(G_1)$  since $G_2$ is suboptimal\n",
    "\n",
    "$f(G_2) \\geq g(G_1) \\geq (f(n) = g(n) + h(n)) $ since $h$ is admissible\n",
    "\n",
    "Since $f(G_2) \\geq f(n)$, *A* will never select $G_2$ for expansion*\n",
    "\n",
    "***Lemma*** A* expands nodes in order of increasing f value.\n",
    "![consistent](images/consistent.jpg)\n",
    "\n",
    "* A heuristic is consistent if $h(n) ‚â§ c(n, a, n') + h(n')$\n",
    "* If **h** is consistent, we have\n",
    "\n",
    "$f(n') = g(n') + h(n')$\n",
    "\n",
    "$= g(n) + c(n, a, n') + h(n')$\n",
    "\n",
    "$\\geq g(n) + h(n) = f(n)$\n",
    "\n",
    "hence f(n) is nondecreasing along any path\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23d9fdd",
   "metadata": {},
   "source": [
    "By far a heuristic is:\n",
    " * **admissible** if $h(n) \\leq h^*(n)$ for all n, where $h^*(n)$ is the true cost from n to goal state\n",
    " * **consistent** if $h(n) \\leq c(n, n') + h(n')$ for all n\n",
    "\n",
    "**Proof strategy for admissibility**: The most common way to prove admissibility is to construct the heuristic as the exact solution to a relaxed problem. A relaxed problem is one where some constraints on the actions are removed, making the path cost to the goal necessarily less than or equal to the cost in the original, more constrained problem. \n",
    "\n",
    "$h(n) = \\text{Optimal Cost in the Relaxed Problem} \\le \\text{Optimal Cost in the Original Problem} = h^*(n)$\n",
    "\n",
    "A heuristic $h(n)$ is consistent if, for every node $n$ and every successor $n'$ generated by an action with $\\text{cost}(n, n')$, the estimated cost to the goal from $n$ is no greater than the cost to get to $n'$ plus the estimated cost from $n'$.The mathematical definition is:\n",
    "\n",
    "$h(n) \\le \\text{cost}(n, n') + h(n')$\n",
    "\n",
    "Rearranging this gives the key insight that the drop in the heuristic value must be less than or equal to the actual move cost:\n",
    "$h(n) - h(n') \\le \\text{cost}(n, n')$\n",
    "\n",
    "**Proof Strategy**: To prove consistency, you must check the inequality for every possible single step from any node $n$ to its neighbor $n'$. This usually involves showing that the move action is constrained such that the heuristic can decrease by at most the step cost.\n",
    "\n",
    "**Key Relationship:** Any consistent heuristic is also admissible (assuming $h(G) = 0$).Proving consistency is sufficient to prove admissibility.\n",
    "\n",
    "----\n",
    "\n",
    "Is A* with graph search always guaranteed to find the optimal solution when using an admissible heuristic?\n",
    "![inconsistent](images/inconsistent.jpg)\n",
    "\n",
    "The optimal path from S to C is S‚àíA‚àíB‚àíC with a total cost of 4. However, with A* graph search:\n",
    "* After starting from S, the options on the frontier are S‚àíA with a cost\n",
    "plus heuristic of 1+3=4, or S‚àíB with a cost plus heuristic of 3+0=3. So,\n",
    "we choose to explore S‚àíB.\n",
    "* Next, the options on the frontier are S‚àíA with a cost plus heuristic of\n",
    "1+3=4, or B‚àíC with a backward path cost of 5. So, we choose to explore\n",
    "S‚àíA.\n",
    "* We've already reached B from the earlier path, so we don't conside \n",
    "edge A‚àíB. Thus, our only option is to explore B‚àíC for a cost of 5.\n",
    "* So, we will choose the path S‚àíB‚àíC with a total cost of 5, which is not\n",
    "optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbb0fc4",
   "metadata": {},
   "source": [
    "### Tree Search\n",
    "* In tree search, a closed set is not needed because nodes are not revisited; there's no way to reach the same node via multiple paths.\n",
    "* The heuristic must be admissible, ensuring that A* will find the optimal path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d073cb",
   "metadata": {},
   "source": [
    "### Graph Search\n",
    "* In graph search, a closed set is essential to\n",
    "avoid revisiting nodes through different paths\n",
    "(e.g., due to loops).\n",
    "* The heuristic must be both admissible and\n",
    "consistent to guarantee that A* finds the\n",
    "optimal path efficiently.\n",
    "* Consistent heuristic guarantees optimal path even without\n",
    "closed set, but reprocessing nodes can be inefficient.\n",
    "* Inconsistent heuristic can lead to excessive node expansions.\n",
    "* Using a closed set with an inconsistent heuristic can lead to\n",
    "suboptimal solutions.\n",
    "* A* with a consistent heuristic is optimally efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e9832d",
   "metadata": {},
   "source": [
    "## 7. Choosing a heuristic\n",
    "\n",
    "E.g., for the 8-puzzle:\n",
    "\n",
    "$h_1(n)$ = number of misplaced tiles (Hamming distance)\n",
    "\n",
    "$h_2(n)$ = total Manhattan distance (i.e., no. of squares from desired location of each tile)\n",
    "\n",
    "$cellDist = |x - x'| + |y - y'|$\n",
    "\n",
    "![puzzle](images/puzzle.jpg)\n",
    "\n",
    "$h_1(S) = 1 + 0 + 1 + 1 + 1 + 0 + 1 + 1 = 6$\n",
    "\n",
    "$h_2(S) = 4 + 0 + 3 + 3 + 1 + 0 + 2 + 1 = 14$\n",
    "\n",
    "*Both Manhattan and Hamming heuristics are admissible and consistent*\n",
    "\n",
    "If $h_2(n) \\geq h_1(n)$ for all $n$ (both admissible) then $h_2$ dominates and is better for search \n",
    "\n",
    "Given any admissible heuristics $h_a$ and $h_b$ , then $h(n)=max(h_a(n), h_b(n))$ is admissible and dominates $h_a, h_b$\n",
    "\n",
    "More formally: \n",
    "\n",
    "The Manhattan distance heuristic, $h_M(n)$, is the sum of the horizontal and vertical distances of every misplaced tile from its goal position.\n",
    "\n",
    "$h_M(n) = \\sum_{\\text{all tiles}} |\\text{current row} - \\text{goal row}| + |\\text{current column} - \\text{goal column}|$. \n",
    "\n",
    "**Admissibility Proof for Manhattan Distance Relaxed Problem:** A tile can be moved to its goal position in one step, regardless of what other tiles or the blank space are doing.\n",
    "1. The true cost, $h^*(n)$, is the number of moves needed to move all tiles to their correct positions without collisions.\n",
    "2. The Manhattan distance $h_M(n)$ calculates the minimum number of unit steps required for each tile individually to reach its goal position, **ignoring the presence of other tiles and the constraint of the blank space**.\n",
    "3. Since the true problem requires a tile to wait for the blank space and potentially move other tiles out of the way, the actual number of moves $h^*(n)$ must be greater than or equal to the total minimum unit steps of all tiles.\n",
    "4. Therefore, $h_M(n)$ is a lower bound on $h^*(n)$:\n",
    "\n",
    "    $\\mathbf{h_M(n) \\le h^*(n)}$\n",
    "\n",
    "**Consistency Proof for Manhattan Distance:** Assume a move from node $n$ to $n'$ by sliding the blank tile, which swaps positions with an adjacent tile, $T$. The step cost is $\\text{cost}(n, n') = 1$.\n",
    "\n",
    "1. The heuristic value $h_M(n)$ changes only for the tile $T$ that was moved. All other tiles' distances remain the same.\n",
    "2. Since tile $T$ was adjacent to the blank space, it moved one step closer to or one step farther from its goal position.\n",
    "3. **Best Case:** If the move brings tile $T$ one step closer to its goal, the Manhattan distance for $T$ decreases by 1.\n",
    "\n",
    "    $\\Delta h_M = -1 \\implies h_M(n) - h_M(n') = 1$\n",
    "\n",
    "4. **Worst Case:** If the move takes tile $T$ one step farther from its goal, the Manhattan distance for $T$ increases by 1.\n",
    "\n",
    "    $\\Delta h_M = +1 \\implies h_M(n) - h_M(n') = -1$\n",
    "\n",
    "5. **No Change Case:** If the move changes only one coordinate (e.g., column) but is one step closer in that coordinate and one step farther in the other (e.g., row), the distance for $T$ stays the same.\n",
    "\n",
    "    $\\Delta h_M = 0 \\implies h_M(n) - h_M(n') = 0$ \n",
    "\n",
    "In all cases, the change in the heuristic value $h_M(n) - h_M(n')$ is $\\le 1$. Since $\\text{cost}(n, n') = 1$, the consistency inequality is satisfied:\n",
    "$h_M(n) - h_M(n') \\le 1 = \\text{cost}(n, n')$\n",
    "\n",
    "$\\mathbf{h_M(n) \\le \\text{cost}(n, n') + h_M(n')}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e055d90c",
   "metadata": {},
   "source": [
    "### Memory-based A*\n",
    "= combination between A* and beam-search, i.e., keeps l nodes in the queue\n",
    "\n",
    "## 8. Iterative deepening A*\n",
    "= combination between A* and iterativedeepening-search, i.e., uses A* evaluation function as a threshold and runs DLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c036d57",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "| Algorithm | Complete | Optimal | Time Complexity | Space Complexity | Data structure|\n",
    " | ----- | ----- | ----- | ----- | ----- | ----- |\n",
    "| Greedy | $No^*$ | $No$ | $O(b^m)$ | $O(b^m)$ | Priority Queue |\n",
    "| Beam Search | $No$ |  $No$   | $O(bml)$ | $O(bl)$ | Limited Priority Queue |  \n",
    "| Hill Climbing | $No$ | $No$ | $O(bm)$ | $O(b)$ | \"Limited Priority Queue\"  |\n",
    "| A* | $Yes^*$  | $Yes$ | $O(b^d)$ | $O(b^d)$ | Priority Queue  |\n",
    "| IDA* | $Yes$ | $Yes$ | $O(b^d)$ | $O(bd)$ | Stack  |\n",
    "\n",
    "A* is often used for the common pathfinding problem in applications such as video games, but was originally designed as a general graph traversal algorithm. It finds applications in diverse problems, including the problem of parsing using stochastic grammars in NLP. Other cases include an Informational search with online learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1a0814",
   "metadata": {},
   "source": [
    "## 10. Simulated Annealing\n",
    "**Idea:** escape local maxima by allowing some \"bad\" moves but gradually decrease their size and frequency\n",
    "\n",
    "Watch the video: [here](https://www.youtube.com/watch?v=C86j1AoMRr0)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

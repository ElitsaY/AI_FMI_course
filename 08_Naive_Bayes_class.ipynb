{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc354dc0",
   "metadata": {},
   "source": [
    "# Lab 08: Naive Bayes Classifier\n",
    "\n",
    "Overview of the lab:\n",
    "* Bayesian Statistics & Bayes' Theorem\n",
    "* Naive Bayes Classifier\n",
    "* Conditional Independence Assumption\n",
    "* Zero-Probability Problem & Laplace Smoothing\n",
    "* Practical Implementation\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a065c3e0",
   "metadata": {},
   "source": [
    "## Introduction to Naive Bayes\n",
    "\n",
    "**Naive Bayes Classifier** is a probabilistic machine learning algorithm based on **Bayes' Theorem**. It is called \"naive\" because it makes a strong assumption that all features are **conditionally independent** given the class label.\n",
    "\n",
    "### Where does it fit in Machine Learning?\n",
    "\n",
    "* **Supervised Learning**: Uses labeled training data in the format `<input data, true label>`\n",
    "* **Classification**: Predicts the class/category of new examples\n",
    "* **Probabilistic Approach**: Unlike KNN (which uses distance), Naive Bayes uses probability theory\n",
    "\n",
    "### Common Applications:\n",
    "* Spam email detection\n",
    "* Sentiment analysis\n",
    "* Document classification\n",
    "* Medical diagnosis\n",
    "* Real-time prediction (fast training and prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b0070f",
   "metadata": {},
   "source": [
    "## Machine Learning Recap\n",
    "\n",
    "### Data Terminology\n",
    "* **Training data**: Data used to train the algorithm\n",
    "* **Validation data**: Data used to tune hyperparameters\n",
    "* **Test data**: Previously unseen data used to test how well the algorithm generalizes predictions\n",
    "* **k-Cross Validation**: Method for finding best results (typically k=5 or k=10)\n",
    "* **Bias**: The model's inability to capture the relationship between data\n",
    "* **Variance**: Difference in prediction on unseen data\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442f19d0",
   "metadata": {},
   "source": [
    "## Bayes' Theorem\n",
    "\n",
    "**Bayes' Theorem** is the foundation of the Naive Bayes classifier. It allows us to calculate the probability of an event based on prior knowledge of conditions related to that event.\n",
    "\n",
    "### Probability Basics\n",
    "\n",
    "For two events A and B:\n",
    "\n",
    "$$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$$\n",
    "\n",
    "$$P(B|A) = \\frac{P(A \\cap B)}{P(A)}$$\n",
    "\n",
    "From these, we get:\n",
    "\n",
    "$$P(A \\cap B) = P(B|A) \\cdot P(A) = P(A|B) \\cdot P(B)$$\n",
    "\n",
    "### Bayes' Theorem Formula\n",
    "\n",
    "Rearranging the above equation gives us **Bayes' Theorem**:\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$$\n",
    "\n",
    "Where:\n",
    "* $P(A|B)$ = **Posterior probability**: Probability of A given B\n",
    "* $P(B|A)$ = **Likelihood**: Probability of B given A  \n",
    "* $P(A)$ = **Prior probability**: Probability of A\n",
    "* $P(B)$ = **Evidence**: Probability of B\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61547d30",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier - General Formula\n",
    "\n",
    "For classes $C_1, C_2, ..., C_K$ and attributes/features $\\vec{x} = (x_1, x_2, ..., x_n)$:\n",
    "\n",
    "### The Classification Formula\n",
    "\n",
    "Using Bayes' Theorem, we want to find:\n",
    "\n",
    "$$P(C_i | x_1, x_2, ..., x_n) = \\frac{P(x_1, x_2, ..., x_n | C_i) \\cdot P(C_i)}{P(x_1, x_2, ..., x_n)}, \\quad i = 1, ..., K$$\n",
    "\n",
    "**What does this mean?** \n",
    "\n",
    "$P(C_i | x_1, x_2, ..., x_n)$ represents: *\"Given that we observe features $x_1, x_2, ..., x_n$, what is the probability that the example belongs to class $C_i$?\"*\n",
    "\n",
    "**Example - Medical Diagnosis:**\n",
    "\n",
    "Suppose we want to diagnose whether a patient has a disease based on symptoms:\n",
    "* Classes: $C_1$ = Has Disease, $C_2$ = Healthy\n",
    "* Features: $x_1$ = Fever (Yes/No), $x_2$ = Cough (Yes/No), $x_3$ = Fatigue (Yes/No)\n",
    "\n",
    "For a patient with Fever=Yes, Cough=Yes, Fatigue=No:\n",
    "* $P(C_1 | \\text{Fever=Yes, Cough=Yes, Fatigue=No})$ = \"What is the probability the patient has the disease, given these symptoms?\"\n",
    "* $P(C_2 | \\text{Fever=Yes, Cough=Yes, Fatigue=No})$ = \"What is the probability the patient is healthy, given these symptoms?\"\n",
    "\n",
    "We classify the patient into whichever class has the higher probability.\n",
    "\n",
    "**Goal**: For a new example with features $\\vec{x}$, we want to find the class $C_i$ that maximizes $P(C_i | \\vec{x})$.\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Computing $P(x_1, x_2, ..., x_n | C_i)$ directly is **computationally expensive** because:\n",
    "* We need to calculate the joint probability of all features\n",
    "* This requires exponentially many combinations\n",
    "* For n features with d values each, we need $d^n$ probabilities!\n",
    "\n",
    "**Example**: With 10 binary features, we'd need $2^{10} = 1024$ probability values per class!\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fbe22e",
   "metadata": {},
   "source": [
    "## The \"Naive\" Assumption - Conditional Independence\n",
    "\n",
    "The **\"naive\"** in Naive Bayes comes from a **strong simplifying assumption**:\n",
    "\n",
    "### Conditional Independence Assumption\n",
    "\n",
    "**All features are conditionally independent given the class.**\n",
    "\n",
    "Mathematically, for a given class $C_i$:\n",
    "\n",
    "$$P(x_j | x_{j+1}, x_{j+2}, ..., x_n, C_i) = P(x_j | C_i)$$\n",
    "\n",
    "This means: *Given we know the class, knowing one feature tells us nothing about another feature.*\n",
    "\n",
    "### Why is this \"Naive\"?\n",
    "\n",
    "This assumption is often **not true in reality**! \n",
    "\n",
    "**Example**: Email spam detection\n",
    "* Feature 1: Email contains word \"free\"\n",
    "* Feature 2: Email contains word \"money\"\n",
    "\n",
    "These features are actually correlated (they often appear together in spam), but Naive Bayes assumes they're independent given the class.\n",
    "\n",
    "### The Benefit\n",
    "\n",
    "Despite being unrealistic, this assumption allows us to **decompose** the joint probability:\n",
    "\n",
    "$$P(x_1, x_2, ..., x_n | C_i) = P(x_1 | C_i) \\cdot P(x_2 | C_i) \\cdot ... \\cdot P(x_n | C_i) = \\prod_{j=1}^{n} P(x_j | C_i)$$\n",
    "\n",
    "This reduces complexity from $d^n$ to just $n \\cdot d$ probability values!\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030e579d",
   "metadata": {},
   "source": [
    "## Naive Bayes - Final Formula\n",
    "\n",
    "Combining Bayes' Theorem with the conditional independence assumption:\n",
    "\n",
    "$$P(C_i | x_1, ..., x_n) \\propto P(C_i) \\cdot \\prod_{j=1}^{n} P(x_j | C_i)$$\n",
    "\n",
    "**Note**: We ignore the denominator $P(x_1, ..., x_n)$ because:\n",
    "* It's the same for all classes $C_i$\n",
    "* We only care about which class has the highest probability\n",
    "* We can compare classes without computing it\n",
    "\n",
    "### Classification Decision\n",
    "\n",
    "To classify a new example $\\vec{x} = (x_1, ..., x_n)$:\n",
    "\n",
    "$$\\hat{C} = \\arg\\max_{C_i} P(C_i) \\cdot \\prod_{j=1}^{n} P(x_j | C_i)$$\n",
    "\n",
    "Choose the class $C_i$ that **maximizes** the expression above.\n",
    "\n",
    "### What We Need to Learn from Training Data\n",
    "\n",
    "1. **Prior probabilities**: $P(C_i)$ for each class\n",
    "   * Simply count: $P(C_i) = \\frac{\\text{count of examples in class } C_i}{\\text{total count of examples}}$\n",
    "\n",
    "2. **Conditional probabilities**: $P(x_j | C_i)$ for each feature value given each class\n",
    "   * Count: $P(x_j | C_i) = \\frac{\\text{count of times feature } x_j \\text{ appears in class } C_i}{\\text{count of examples in class } C_i}$\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60738905",
   "metadata": {},
   "source": [
    "## Example: Email Spam Classification\n",
    "\n",
    "Let's work through a concrete example to understand how Naive Bayes works.\n",
    "\n",
    "### Problem Setup\n",
    "\n",
    "**Classes**: \n",
    "* $C_1$ = Spam\n",
    "* $C_2$ = Not Spam (Ham)\n",
    "\n",
    "**Features** (words in email):\n",
    "* $x_1$ = contains \"free\"\n",
    "* $x_2$ = contains \"money\"\n",
    "* $x_3$ = contains \"meeting\"\n",
    "\n",
    "### Training Data\n",
    "\n",
    "Suppose we have 100 emails:\n",
    "* 40 are spam\n",
    "* 60 are ham\n",
    "\n",
    "**Prior probabilities**:\n",
    "* $P(\\text{Spam}) = 40/100 = 0.4$\n",
    "* $P(\\text{Ham}) = 60/100 = 0.6$\n",
    "\n",
    "**Conditional probabilities** (counted from training data):\n",
    "\n",
    "| Word | P(word \\| Spam) | P(word \\| Ham) |\n",
    "|------|-----------------|----------------|\n",
    "| \"free\" | 0.30 | 0.05 |\n",
    "| \"money\" | 0.25 | 0.10 |\n",
    "| \"meeting\" | 0.05 | 0.40 |\n",
    "\n",
    "### Classification Task\n",
    "\n",
    "We receive a new email with words: **\"free\"** and **\"money\"** (but not \"meeting\")\n",
    "\n",
    "**Calculate**:\n",
    "\n",
    "For Spam:\n",
    "$$P(\\text{Spam} | \\text{free, money}) \\propto P(\\text{Spam}) \\cdot P(\\text{free}|\\text{Spam}) \\cdot P(\\text{money}|\\text{Spam})$$\n",
    "$$= 0.4 \\times 0.30 \\times 0.25 = 0.03$$\n",
    "\n",
    "For Ham:\n",
    "$$P(\\text{Ham} | \\text{free, money}) \\propto P(\\text{Ham}) \\cdot P(\\text{free}|\\text{Ham}) \\cdot P(\\text{money}|\\text{Ham})$$\n",
    "$$= 0.6 \\times 0.05 \\times 0.10 = 0.003$$\n",
    "\n",
    "**Decision**: Since $0.03 > 0.003$, classify as **Spam** ✉️\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6e5205",
   "metadata": {},
   "source": [
    "## Zero-Probability Problem\n",
    "\n",
    "### The Problem\n",
    "\n",
    "**What happens if a feature value never appears in the training data for a particular class?**\n",
    "\n",
    "**Example**: \n",
    "* Word \"discount\" never appeared in any spam email in training data\n",
    "* $P(\\text{\"discount\"} | \\text{Spam}) = 0/40 = 0$\n",
    "\n",
    "**Issue**:\n",
    "$$P(\\text{Spam} | \\text{contains \"discount\"}) \\propto P(\\text{Spam}) \\cdot P(\\text{\"discount\"}|\\text{Spam}) \\cdot ...$$\n",
    "$$= 0.4 \\times \\mathbf{0} \\times ... = \\mathbf{0}$$\n",
    "\n",
    "The entire probability becomes **zero**, regardless of other features! This is **catastrophic**.\n",
    "\n",
    "### Why is this a problem?\n",
    "\n",
    "* **Lack of training data**: Just because we didn't see \"discount\" in spam during training doesn't mean it can NEVER appear in spam\n",
    "* **Probability multiplication**: A single zero makes the entire product zero\n",
    "* **Loss of information**: All other features are ignored\n",
    "\n",
    "This is called the **Zero-Probability Problem** or **Zero-Frequency Problem**.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac297dd",
   "metadata": {},
   "source": [
    "## Solution: Laplace Smoothing\n",
    "\n",
    "**Laplace Smoothing** (also called **additive smoothing**) solves the zero-probability problem by adding a small constant to all counts.\n",
    "\n",
    "### The Formula\n",
    "\n",
    "Instead of:\n",
    "$$P(x_j | C_i) = \\frac{\\text{Count}(x_j, C_i)}{\\text{Count}(C_i)}$$\n",
    "\n",
    "We use:\n",
    "$$P(x_j | C_i) = \\frac{\\text{Count}(x_j, C_i) + \\alpha}{\\text{Count}(C_i) + \\alpha \\cdot V}$$\n",
    "\n",
    "Where:\n",
    "* $\\alpha$ = smoothing parameter (typically $\\alpha = 1$, called **Laplace smoothing** or **add-one smoothing**)\n",
    "* $V$ = number of different possible values for the feature (vocabulary size)\n",
    "* $\\text{Count}(x_j, C_i)$ = number of times feature value $x_j$ appears in class $C_i$\n",
    "* $\\text{Count}(C_i)$ = total count of all feature values in class $C_i$\n",
    "\n",
    "### Why Does This Work?\n",
    "\n",
    "1. **Guarantees non-zero probabilities**: Even if $\\text{Count}(x_j, C_i) = 0$, we get $P(x_j | C_i) = \\frac{\\alpha}{\\text{Count}(C_i) + \\alpha \\cdot V} > 0$\n",
    "\n",
    "2. **Maintains probability distribution**: The probabilities still sum to 1\n",
    "\n",
    "3. **Minimal impact on frequent features**: For features that appear often, adding $\\alpha$ has negligible effect\n",
    "\n",
    "### Example with Laplace Smoothing\n",
    "\n",
    "Suppose for word \"discount\":\n",
    "* Never appeared in 40 spam emails: Count(\"discount\", Spam) = 0\n",
    "* Vocabulary size: V = 1000 words\n",
    "* $\\alpha = 1$\n",
    "\n",
    "**Without smoothing**: \n",
    "$$P(\\text{\"discount\"} | \\text{Spam}) = \\frac{0}{40} = 0$$\n",
    "\n",
    "**With Laplace smoothing**:\n",
    "$$P(\\text{\"discount\"} | \\text{Spam}) = \\frac{0 + 1}{40 + 1 \\times 1000} = \\frac{1}{1040} \\approx 0.00096$$\n",
    "\n",
    "Now the probability is small but **not zero**!\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed07346",
   "metadata": {},
   "source": [
    "## Advantages and Disadvantages of Naive Bayes\n",
    "\n",
    "| Advantages ✅ | Disadvantages ❌ |\n",
    "|--------------|------------------|\n",
    "| **Fast training and prediction**: Only needs to count frequencies | **Naive independence assumption**: Assumes features are independent, which is often unrealistic |\n",
    "| **Simple to implement**: Easy to understand and code | **Zero-frequency problem**: Requires smoothing techniques |\n",
    "| **Works well with high-dimensional data**: Scales well with number of features | **Poor probability estimates**: The actual probability values can be inaccurate (though classification can still be good) |\n",
    "| **Requires small amount of training data**: Can work with limited data | **Sensitive to irrelevant features**: Irrelevant features can affect predictions |\n",
    "| **Handles missing values well**: Can easily skip missing features | **Cannot learn feature interactions**: Misses relationships between features |\n",
    "| **Probabilistic predictions**: Provides probability estimates for classes | **Continuous features**: Requires assumptions about distribution (e.g., Gaussian) |\n",
    "\n",
    "### When to Use Naive Bayes?\n",
    "\n",
    "**Good for**:\n",
    "* Text classification (spam detection, sentiment analysis, document categorization)\n",
    "* Real-time prediction (very fast)\n",
    "* Multi-class prediction\n",
    "* When features are relatively independent\n",
    "* When you have limited training data\n",
    "\n",
    "**Not ideal for**:\n",
    "* Problems with strong feature dependencies\n",
    "* When you need accurate probability estimates\n",
    "* Regression problems (it's a classifier)\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2880d99",
   "metadata": {},
   "source": [
    "## Summary: Key Takeaways\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "1. **Naive Bayes** is a **probabilistic classifier** based on **Bayes' Theorem**\n",
    "\n",
    "2. **The \"Naive\" Assumption**: All features are **conditionally independent** given the class\n",
    "   * This is usually **not true** in reality\n",
    "   * But it makes computation **tractable** and often works well in practice\n",
    "\n",
    "3. **Classification Formula**:\n",
    "   $$\\hat{C} = \\arg\\max_{C_i} P(C_i) \\cdot \\prod_{j=1}^{n} P(x_j | C_i)$$\n",
    "\n",
    "4. **Training**: Just count frequencies\n",
    "   * $P(C_i)$ = fraction of training examples in class $C_i$\n",
    "   * $P(x_j | C_i)$ = fraction of times feature $x_j$ appears in class $C_i$\n",
    "\n",
    "5. **Zero-Probability Problem**: Solved by **Laplace Smoothing**\n",
    "   $$P(x_j | C_i) = \\frac{\\text{Count}(x_j, C_i) + \\alpha}{\\text{Count}(C_i) + \\alpha \\cdot V}$$\n",
    "\n",
    "### Why Naive Bayes Works Despite Being \"Naive\"\n",
    "\n",
    "Even though the independence assumption is often violated:\n",
    "* It simplifies computation dramatically\n",
    "* The classification decision often remains correct even if probability estimates are wrong\n",
    "* Works surprisingly well in many real-world applications (especially text classification)\n",
    "\n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34ab1073",
   "metadata": {},
   "source": [
    "# Lab09: Metrics\n",
    "\n",
    "Overview of the lab:\n",
    "1. kNN implementation\n",
    "2. Accuracy \n",
    "3. Cross Validation\n",
    "4. Confusion Matrix\n",
    "5. Calculating Presion Recall\n",
    "6. Recap on Bias and Variance\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ce3dbe",
   "metadata": {},
   "source": [
    "## 1. kNN implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81842a9d",
   "metadata": {},
   "source": [
    "## 2. Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f93d34",
   "metadata": {},
   "source": [
    "## 3. Cross validation \n",
    "\n",
    "Cross-validation is a statistical method used to **estimate the skill** of a machine learning model on independent data.\n",
    "\n",
    "### The Core Problem it Solves:\n",
    "\n",
    "When training an AI model, you want it to learn the general patterns in your data, not just memorize the specific examples.\n",
    "\n",
    "* **Overfitting:** This happens when a model learns the training data too well, including the noise and outliers, and performs poorly on new data. Cross-validation helps detect and prevent this.\n",
    "* **Data Usage:** It allows you to use your entire dataset for both training and validation by cycling through different data subsets.\n",
    "\n",
    "\n",
    "### ðŸ› ï¸ How it Works: The K-Fold Method\n",
    "\n",
    "The most common form is **k-Fold Cross-Validation**. Here is a step-by-step breakdown:\n",
    "\n",
    "1.  **Divide the Data:** The entire dataset is randomly split into $k$ equally sized segments, called \"folds.\" (A common choice for $k$ is 5 or 10).\n",
    "2.  **Iterate and Train:** The process repeats $k$ times (or $k$ \"folds\"). In each iteration:\n",
    "    * One fold is set aside to be the test/validation set.\n",
    "    * The remaining $k-1$ folds are combined to form the training set.\n",
    "    * The model is trained on the training set and then evaluated on the holdout test set.\n",
    "3.  **Aggregate Results:** After all $k$ iterations are complete (and every fold has been used exactly once as the test set), the $k$ evaluation scores (e.g., accuracy, error rate) are averaged.\n",
    "\n",
    "$$\\text{Model Performance} = \\frac{\\text{Score}_1 + \\text{Score}_2 + \\dots + \\text{Score}_k}{k}$$\n",
    "\n",
    "This final average score provides a much more *stable and unbiased* estimate of the model's true predictive power on unseen data.\n",
    "\n",
    "\n",
    "### ðŸ“‹ Common Types of Cross-Validation\n",
    "\n",
    "| Technique | Description | Best Used When... |\n",
    "| :--- | :--- | :--- |\n",
    "| **k-Fold CV** | Splits data into $k$ folds, cycling through each as the test set. | Most general use; for a balanced and robust estimate. |\n",
    "| **Leave-One-Out CV (LOOCV)** | $k$ equals the number of data points ($n$); $n-1$ are training, 1 is testing. | Dataset is very small, but computationally expensive for large data. |\n",
    "| **Stratified k-Fold CV** | Ensures each fold has the same proportion of classes as the entire dataset. | Dealing with **imbalanced datasets** (e.g., rare disease detection). |\n",
    "\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "Cross-validation is a critical tool for any AI practitioner because it:\n",
    "\n",
    "* **Estimates Generalization:** Gives you confidence that your model will work in the real world.\n",
    "* **Optimizes Hyperparameters:** It's used to compare different models or find the best settings (hyperparameters) for a single model.\n",
    "* **Reduces Bias:** Ensures the model's performance isn't dependent on a single, lucky random split of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143e1282",
   "metadata": {},
   "source": [
    "## 4. The Confusion Matrix\n",
    "\n",
    "### Structure of the Confusion Matrix\n",
    "\n",
    "The matrix's rows represent what the machine learning algorithm **predicted**, and the columns represent the **known truth**.\n",
    "\n",
    "For a binary classification problem (e.g., predicting heart disease or no heart disease), a 2x2 matrix contains four key outcomes:\n",
    "\n",
    "* **True Positives (TP)**: Correctly identified positive cases. (The patient *had* heart disease, and the algorithm said they *did*.)\n",
    "* **True Negatives (TN)**: Correctly identified negative cases. (The patient *did not have* heart disease, and the algorithm said they *did not*.)\n",
    "* **False Positives (FP)**: Incorrectly identified positive cases (Type I Error). (The patient *did not have* heart disease, but the algorithm said they *did*â€”a **false alarm**.)\n",
    "* **False Negatives (FN)**: Incorrectly identified negative cases (Type II Error). (The patient *had* heart disease, but the algorithm said they *did not*â€”a **missed case**.)\n",
    "\n",
    "**Positives / Negatives** -> the prediction of the algorithm\n",
    "\n",
    "![confusion_matrix](images/confusion_matrix.png)\n",
    "\n",
    "### Interpretation and Application\n",
    "\n",
    "1.  **Correct vs. Incorrect Classification:** The numbers along the **diagonal** of the matrix (TP and TN) represent the samples that were **correctly classified**. The numbers off the diagonal (FP and FN) are the samples where the algorithm made an error.\n",
    "\n",
    "2.  **Model Comparison:** The confusion matrix allows researchers to compare the performance of different machine learning methods (e.g., Random Forest vs. K-Nearest Neighbors) to determine which is a better fit for the data.\n",
    "\n",
    "3.  **Multi-Class Classification:** The size of the confusion matrix is determined by the number of outcomes being \n",
    "predicted. If you have three categories, you will have a 3x3 matrix; if you have $N$ categories, you will have an $N$ x $N$ matrix. In multi-class matrices, the diagonal still represents correct classifications, and all other cells represent errors.\n",
    "\n",
    "In summary, a confusion matrix provides a clear breakdown of what a machine learning algorithm got right and what it got wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65a4222",
   "metadata": {},
   "source": [
    "## 5. Calculating Sensitivity and Specificity (Precision and Recall)\n",
    "\n",
    "These metrics measure how well a model correctly identifies positive and negative cases in a **binary classification** (2x2) problem.\n",
    "\n",
    "| Metric | Formula | Interpretation |\n",
    "| :--- | :--- | :--- |\n",
    "| **Sensitivity (Recall)** | $\\frac{\\text{True Positives}}{(\\text{True Positives} + \\text{False Negatives})}$ | The percentage of all actual **positive** cases that were **correctly identified**. It's the ability of the test to find the sick people. |\n",
    "| **Specificity** | $\\frac{\\text{True Negatives}}{(\\text{True Negatives} + \\text{False Positives})}$ | The percentage of all actual **negative** cases that were **correctly identified**. It's the ability of the test to rule out the non-sick people. |\n",
    "\n",
    "If we are searching for \"People with heart disease\" then Sensitivity tracks the percentage of patients with heart disease that were corrctly identified. \n",
    "\n",
    "![sensitivity](images/sensitivity.jpg)\n",
    "\n",
    "### Model Selection and Interpretation\n",
    "\n",
    "Sensitivity and Specificity help determine which model is best, based on which type of error is more acceptable:\n",
    "\n",
    "* **Higher Sensitivity** is preferred when **correctly identifying positives** is most important (i.e., you want to minimize **False Negatives** or \"missed cases\"). For example, in a medical test for a dangerous disease.\n",
    "* **Higher Specificity** is preferred when **correctly identifying negatives** is most important (i.e., you want to minimize **False Positives** or \"false alarms\").\n",
    "\n",
    "\n",
    "### Multi-Class Classification\n",
    "\n",
    "For confusion matrices with three or more classes (e.g., a 3x3 matrix), you must calculate a **separate sensitivity and specificity for each category**.\n",
    "\n",
    "To calculate the metrics for a specific target class:\n",
    "\n",
    "* **True Positives (TP)** are the correct predictions for the target class.\n",
    "* **False Negatives (FN)** are the cases that *belonged* to the target class but were predicted as any *other* class.\n",
    "* **True Negatives (TN)** are all cases correctly predicted to be any *other* class.\n",
    "* **False Positives (FP)** are the cases that *belonged* to an *other* class but were predicted as the **target class**.\n",
    "\n",
    "![3matrix](images/3matrix.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1be12b",
   "metadata": {},
   "source": [
    "## 6. Bias and Variance\n",
    "\n",
    "### 1. Bias\n",
    "\n",
    "**Bias** is the inability of a machine learning method to accurately capture the true underlying relationship in the data. Bias is the error introduced by approximating a real-world problem (which may be very complex) with a much simpler model. It measures how far the average prediction of your model is from the correct value.\n",
    "\n",
    "* **High Bias (Simple Model):** A model that is too simple (e.g., linear regression fitting a straight line to a curved relationship) cannot represent the complexity of the data, regardless of how well it's fit to the training set. It lacks the flexibility to bend to the true relationship.\n",
    "\n",
    "### 2. Variance\n",
    "\n",
    "**Variance** is the difference in a model's performance (its fit) across different data sets.\n",
    "\n",
    "* **High Variance (Complex Model):** A model that is too complex (e.g., a squiggly line) is highly flexible and can fit the training data almost perfectly (low error). However, this flexibility means it is too sensitive to the specific data points in the training set and performs poorly on new, unseen data (the testing set).\n",
    "    * A high-variance model that fits the training data well but fails on the testing data is called **overfit**.\n",
    "\n",
    "### 3. The Bias-Variance Tradeoff\n",
    "bias = incapability to capture the underlying relationship in the data.\n",
    "\n",
    "```capable (Low Bias) < (-1)------(0)------(1) > incapable (High Bias)```\n",
    "                \n",
    "variance = sensitivity to different data; difference in model performence for different data sets\n",
    "\n",
    "```consistent (Low Variance) < (-1)------(0)------(1) > very different (High Variance)```\n",
    "                \n",
    "\n",
    "The **ideal algorithm** must have **low bias** and **low variance**. This is achieved by finding the  sweet spotâ€”a model complexity that is flexible enough to capture the true relationship but simple enough to produce consistent predictions across different datasets.\n",
    "\n",
    "* **Overfitting:** Occurs when a model learns the random noise and fluctuations in the training data instead of the true patterns, resulting in good training performance but poor generalization. The model has high variance.\n",
    "\n",
    "* **Underfitting:** Occurs when a model is too simple to capture the important patterns, resulting in poor performance on both training and test data. The model has *high bias*.\n",
    "\n",
    "* Low Complexity Model => High Bias, Low Variance (Underfitting)\n",
    "* High Complexity Mode => Low Bias, High Variance (Overfitting)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
